{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhQHueVXBZe9Yp5MP9Sp0f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/medYousseffathallah/demand-forecast/blob/main/Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Byspark"
      ],
      "metadata": {
        "id": "RdSQ-EZQxJNv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jyBRMIXwtcg"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# initialize a spark session"
      ],
      "metadata": {
        "id": "5cIPYX2rxXnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " spark=SparkSession.builder.appName('practice').getOrCreate()"
      ],
      "metadata": {
        "id": "LFmC7MHSxc3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create a DataFrame"
      ],
      "metadata": {
        "id": "T3uRQ5QBxpiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "census_df=spark.read.csv('census.csv',[\"gender\",\"age\",\"zipcode\",\"marriage_status\"])*\n",
        "#show the DataFrame\n",
        "census_df.show()\n",
        "#print the DF scema\n",
        "census_df.printSchema()\n",
        "#.count() will t=return the total row number in the DF sum() min() max()\n",
        "row_count=census_df.count()\n",
        "print(f\"Total number of rows in the DataFrame: {row_count}\")\n",
        "#groupby() allows the use sql_like aggregations .select() .filter() .agg()\n",
        "c*ensus_df.groupBy('marriage_status').count().show()\n",
        "#usage of filter and select\n",
        "filtered_census_df=census_df.filter(census_df['age']>30).select('gender','age')\n",
        "filtered_census_df.show()"
      ],
      "metadata": {
        "id": "B_MY9Iuixs54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import\n",
        "from pyspark.sql.types import StructField,StringType,IntegerType,StructType,ArrayType\n",
        "#construct the schema\n",
        "schema=StructType([\n",
        "    StructField('gender',StringType(),True),\n",
        "    StructField('age',IntegerType(),True),\n",
        "    StructField('zipcode',IntegerType(),True),\n",
        "    StructField('marriage_status',ArrayType(StringType()),True)\n",
        "])\n",
        "#set the schema\n",
        "df=spark.createDataFrame(data,schema=schema)\n",
        "#print the schema\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "K1eodVey-MEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UDF functions"
      ],
      "metadata": {
        "id": "VuujXIOcOR6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pandas Udf\n",
        "@pandas_udf('long')\n",
        "def pandas_plus_one(series:pd.Series)->pd.Series:\n",
        "  return series+1"
      ],
      "metadata": {
        "id": "8bsvpXbTOVBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#sort and dropping missing values"
      ],
      "metadata": {
        "id": "IHnXYGLdBSR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sort using the age column\n",
        "df.sort(\"age\",ascending=False).show()\n",
        "#drop missing values\n",
        "df.na.drop().show()"
      ],
      "metadata": {
        "id": "5ngIGOYiBWtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#usecase data manipulation"
      ],
      "metadata": {
        "id": "pcL4FevkX9xK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df=spark.read.csv('/content/Coffee_Shop_Sales.csv',header=True,inferSchema=True)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "vrRXWIo63M7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "s_4FR56g3NBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=df.filter(df[\"store_id\"]==5)\n",
        "df1.count()"
      ],
      "metadata": {
        "id": "MvhBq8YlEOac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_df=df1.select(\n",
        "    \"transaction_id\",\n",
        "    \"transaction_date\",\n",
        "    \"transaction_qty\",\n",
        "    \"store_id\",\n",
        "    \"product_id\",\n",
        "    \"unit_price\",\n",
        "    \"product_category\",\n",
        ")\n",
        "store_df.show()\n"
      ],
      "metadata": {
        "id": "gWId-HM4Ogbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum,when,col\n",
        "store_df_count=store_df.select(\n",
        "    sum(when(col(\"transaction_id\").isNotNull(),1).otherwise(0)).alias(\"transaction_id\"),\n",
        "    sum(when(col(\"transaction_date\").isNotNull(),1).otherwise(0)).alias(\"transaction_time\"),\n",
        "    sum(when(col(\"transaction_qty\").isNotNull(),1).otherwise(0)).alias(\"transaction_qty\"),\n",
        "    sum(when(col(\"store_id\").isNotNull(),1).otherwise(0)).alias(\"store_id\"),\n",
        "    sum(when(col(\"product_id\").isNotNull(),1).otherwise(0)).alias(\"product_id\"),\n",
        "    sum(when(col(\"unit_price\").isNotNull(),1).otherwise(0)).alias(\"unit_price\"),\n",
        "    sum(when(col(\"product_category\").isNotNull(),1).otherwise(0)).alias(\"product_category\"),\n",
        ").show()\n",
        "\n",
        "store_df.printSchema()"
      ],
      "metadata": {
        "id": "0FAQpBoCSp9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum,when,col,round\n",
        "\n",
        "\n",
        "store_df_complete=store_df.withColumn(\n",
        "    \"total_sales\",\n",
        "    round(col(\"transaction_qty\") * col(\"unit_price\"), 1)\n",
        "\n",
        ")\n",
        "store_df_complete.show()\n"
      ],
      "metadata": {
        "id": "ubOjhZFtbl4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_df_demo=store_df_complete.drop(\"unit_price\")\n",
        "store_df_demo.show()"
      ],
      "metadata": {
        "id": "JJzTaOiT1vF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " store_df_demo.describe(['transaction_date']).show()"
      ],
      "metadata": {
        "id": "br2luP4Y3rds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "sample_df=store_df_demo.filter(store_df_demo['store_id']==5).select(['total_sales','transaction_date']).sample(withReplacement=False,fraction=0.9,seed=42)\n",
        "sample_df.count()\n",
        "pandas_df=sample_df.toPandas()\n",
        "\n",
        "# Group by month or week and sum sales\n",
        "pandas_df['transaction_date'] = pd.to_datetime(pandas_df['transaction_date'])\n",
        "pandas_df = pandas_df.set_index('transaction_date')\n",
        "monthly_df = pandas_df.resample('ME').sum()\n",
        "monthly_df = monthly_df.reset_index()  # makes 'transaction_date' a column again\n",
        "sns.lineplot(x='transaction_date', y='total_sales', data=monthly_df)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "pandas_df.tail()\n",
        "\n"
      ],
      "metadata": {
        "id": "DH1P_egeblvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_df_demo.drop(\"store_id\",\"transaction_id\")"
      ],
      "metadata": {
        "id": "99u7IRZNlsTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final=store_df_demo.drop(\"product_category\",\"transaction_id\",\"store_id\")\n",
        "final.show()"
      ],
      "metadata": {
        "id": "Y45UGF5266aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data date transform from String to date  "
      ],
      "metadata": {
        "id": "GR7Xy68YIGf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, col\n",
        "from pyspark.sql.functions import month, dayofweek, when\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql.functions import sum as spark_sum\n",
        "\n",
        "# Convert transaction_date string  into a DateType\n",
        "finalized =final.withColumn(\"date\", to_date(col(\"transaction_date\"), \"M/d/yyyy\"))\n",
        "\n",
        "# Extract month (1 = Jan, 6 = Jun)\n",
        "finalized = finalized.withColumn(\"month\", month(col(\"date\")))\n",
        "\n",
        "# Day of week (1 = Sunday, 7 = Saturday)\n",
        "finalized = finalized.withColumn(\"day_of_week\", dayofweek(col(\"date\")))\n",
        "\n",
        "# Weekend flag (1 if Saturday or Sunday, else 0)\n",
        "finalized = finalized.withColumn(\"is_weekend\", when((col(\"day_of_week\") == 1) | (col(\"day_of_week\") == 7), 1).otherwise(0))\n",
        "\n",
        "#holiday list (rass 3am/3id thawra/3id este9lal/3id chouhada/3id 3omal)\n",
        "holidays = [\"2023-01-01\",\"2023-01-14\" ,\"2023-03-20\", \"2023-04-09\",\"2023-05-01\"]\n",
        "\n",
        "# Add holiday\n",
        "finalized = finalized.withColumn(\"is_holiday\", when(col(\"date\").isin(holidays), 1).otherwise(0))\n",
        "\n",
        "daily_df = finalized.groupBy(\"date\", \"month\", \"day_of_week\", \"is_weekend\", \"is_holiday\").agg(spark_sum(\"transaction_qty\").alias(\"daily_demand\")).orderBy(\"date\")\n",
        "finalized.show()\n",
        "daily_df.show(10)"
      ],
      "metadata": {
        "id": "bRqfhPRqIOAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modal training :\n",
        "# =========================\n",
        "# 1) Imports\n",
        "# =========================\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import DateType, IntegerType\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# =========================\n",
        "# 2) Ensure types and minimal cleaning\n",
        "# =========================\n",
        "# If transaction_date is string like \"1/1/2023\", convert to DateType\n",
        "# Adjust the format to match your actual string format (here we use d/M/yyyy)\n",
        "finalized = finalized.withColumn(\n",
        "    \"date\",\n",
        "    F.coalesce(\n",
        "        F.col(\"transaction_date\").cast(DateType()),\n",
        "        F.to_date(\"transaction_date\", \"d/M/yyyy\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Basic sanity checks\n",
        "required = [\"date\", \"transaction_qty\", \"product_id\", \"day_of_week\", \"is_weekend\", \"is_holiday\"]\n",
        "missing = [c for c in required if c not in finalized.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "# Drop rows with nulls in key fields\n",
        "finalized =finalized.dropna(subset=[\"date\", \"transaction_qty\", \"product_id\", \"day_of_week\", \"is_weekend\", \"is_holiday\"])\n",
        "\n",
        "# =========================\n",
        "# 3) Time-based train/test split (no leakage)\n",
        "#    - Train: everything up to (max_date - 14 days)\n",
        "#    - Test: last 14 days\n",
        "# =========================\n",
        "from datetime import timedelta\n",
        "\n",
        "# max_date is already collected as a Python date from Spark\n",
        "max_date = finalized.select(F.max(\"date\").alias(\"max_date\")).collect()[0][\"max_date\"]\n",
        "\n",
        "# subtract 14 days using timedelta\n",
        "cutoff = max_date - timedelta(days=14)\n",
        "\n",
        "# now use cutoff in your filter\n",
        "train_df = finalized.where(F.col(\"date\") <= F.lit(cutoff))\n",
        "test_df  = finalized.where(F.col(\"date\") >  F.lit(cutoff))\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 4) ML Pipeline: product encoding + assemble + model\n",
        "# =========================\n",
        "product_indexer = StringIndexer(inputCol=\"product_id\", outputCol=\"product_id_idx\", handleInvalid=\"keep\")\n",
        "product_ohe     = OneHotEncoder(inputCols=[\"product_id_idx\"], outputCols=[\"product_id_ohe\"])\n",
        "\n",
        "feature_cols = [\"product_id_ohe\", \"day_of_week\", \"is_weekend\", \"is_holiday\"]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "gbt = GBTRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"transaction_qty\",\n",
        "    maxIter=120,\n",
        "    maxDepth=6,\n",
        "    stepSize=0.1,\n",
        "    subsamplingRate=0.8,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=[product_indexer, product_ohe, assembler, gbt])\n",
        "\n",
        "# 5) Train\n",
        "\n",
        "model = pipeline.fit(train_df)\n",
        "\n",
        "# 6) Evaluate\n",
        "pred_test = model.transform(test_df)\n",
        "\n",
        "rmse = RegressionEvaluator(labelCol=\"transaction_qty\", predictionCol=\"prediction\", metricName=\"rmse\").evaluate(pred_test)\n",
        "mae  = RegressionEvaluator(labelCol=\"transaction_qty\", predictionCol=\"prediction\", metricName=\"mae\").evaluate(pred_test)\n",
        "r2   = RegressionEvaluator(labelCol=\"transaction_qty\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(pred_test)\n",
        "\n",
        "print(f\"Test RMSE: {rmse:.3f}\")\n",
        "print(f\"Test MAE : {mae:.3f}\")\n",
        "print(f\"Test R^2 : {r2:.3f}\")\n",
        "\n",
        "# Optional: see a few predictions\n",
        "pred_test.select(\"date\", \"product_id\", \"transaction_qty\", F.col(\"prediction\").alias(\"pred_qty\")).orderBy(\"date\", \"product_id\").show(20, truncate=False)\n",
        "\n",
        "\n",
        "# 7) Forecast next N days per product\n",
        "N_DAYS = 14\n",
        "\n",
        "# Create future date list (next day after max_date → next N days)\n",
        "from datetime import timedelta\n",
        "future_python_dates = [(max_date + timedelta(days=i+1),) for i in range(N_DAYS)]\n",
        "future_dates_df = spark.createDataFrame(future_python_dates, [\"date\"])\n",
        "\n",
        "# Build calendar features for the future dates\n",
        "# dayofweek in Spark: 1=Sun, ..., 7=Sat\n",
        "future_feat = (\n",
        "    future_dates_df\n",
        "    .withColumn(\"day_of_week\", F.dayofweek(\"date\") - 2)      # optional: shift to 0=Mon..6=Sun if you prefer\n",
        "    .withColumn(\"day_of_week\", F.when(F.col(\"day_of_week\") < 0, F.col(\"day_of_week\")+7).otherwise(F.col(\"day_of_week\")))\n",
        "    .withColumn(\"is_weekend\", F.when(F.dayofweek(\"date\").isin(1,7), F.lit(1)).otherwise(F.lit(0)))\n",
        ")\n",
        "\n",
        "# Add holidays for the future horizon (replace with your real list or a holidays table join)\n",
        "future_holidays = []  # e.g., [\"2023-07-25\", ...] as strings\n",
        "future_feat = future_feat.withColumn(\"is_holiday\", F.when(F.col(\"date\").cast(\"string\").isin(future_holidays), 1).otherwise(0))\n",
        "\n",
        "# Cross with all products to get (product_id x future_date)\n",
        "products_df = df.select(\"product_id\").distinct()\n",
        "future_scoring_df = (\n",
        "    products_df.crossJoin(future_feat)\n",
        "    # Ensure the same column names the pipeline expects\n",
        "    .select(\"product_id\", \"date\", \"day_of_week\", \"is_weekend\", \"is_holiday\")\n",
        ")\n",
        "\n",
        "# Score future demand\n",
        "future_pred = model.transform(future_scoring_df) \\\n",
        "    .select(\n",
        "        \"date\", \"product_id\",\n",
        "        F.col(\"prediction\").alias(\"predicted_transaction_qty\")\n",
        "    ) \\\n",
        "    .orderBy(\"date\", \"product_id\")\n",
        "\n",
        "future_pred.show(50, truncate=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4Kxfnoo99DTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum,when,col,round\n",
        "\n",
        "future_pred=future_pred.withColumn(\n",
        "    \"predicted_transaction_qty\",\n",
        "    round(col(\"predicted_transaction_qty\"))\n",
        "\n",
        ")\n",
        "future_pred.show(150, truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "-b3kp7n5_HQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "clFauDvNN7jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stream processing\n"
      ],
      "metadata": {
        "id": "FliHXiqxGTFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a streaming DataFrame from kafka\n",
        "df= spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\",\"localhost:port\").option(\"subscribe\",\"test\").load()\n",
        "#Write Streaming results to kafka\n",
        "query=df.writeStream.format(\"kafka\").outputMode(\"append\").option(\"kafka.bootstrap.servers\",\"localhost:port\").option(\"topic\",\"test\").start()\n",
        "#build streaming DataFrame\n",
        "df=spark.readStream.option(\"maxFilesPerTrigger\",1).format(\"delta\").load()\n",
        "#apply some transformations, producing new strealing DataFrame\n",
        "from pyspark.sql.functions import col\n",
        "email_traffic_df=(df.filter(col(\"traffic_source\"))==\"email\")\n",
        "#write streaming query results\n",
        "email_query=email_traffic_df.writeStream.format(\"delta\").outputMode(\"append\").option(\"checkpointLocation\",\"/tmp/checkpoints/email\").queryName(\"email_traffic\").trigger(processingTime='1 second').start()\n",
        "#stop the streaming query\n",
        "email_query.stop()\n",
        "#wait for termination\n",
        "email_query.awaitTermination()\n",
        "\n",
        "\n",
        "#####Triggers\n",
        "\n",
        "#default when no trigger is specified\n",
        "df.writeStream.start()\n",
        "#process batches at fixed intervals\n",
        "df.writeStream.trigger(processingTime=\"2 minutes\").start()\n",
        "#one time processing of available data\n",
        "df.writingStream.trigger(availableNow=True).start()\n",
        "\n",
        "\n",
        "\n",
        "#### Monitoring via Spark UI\n",
        "\n",
        "\n",
        "#check if DataFrame is streaming\n",
        "df.isStreaming #returns True/False\n",
        "#get unique identifier for query\n",
        "streaming_query.id\n",
        "\n",
        "#fet curretn state for query\n",
        "\n",
        "streaming_query.status\n",
        "#monitor streaming query progress\n",
        "streaming_query.lastProgress\n",
        "#stop the streaming query\n",
        "streaming_query.stop()\n"
      ],
      "metadata": {
        "id": "FxRFgNZRGYB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Define the updated schema\n",
        "schema = StructType([\n",
        "    StructField(\"transaction_id\", StringType(), True),\n",
        "    StructField(\"transaction_date\", StringType(), True),\n",
        "    StructField(\"transaction_time\", TimestampType(), True),\n",
        "    StructField(\"transaction_qty\", StringType(), True),\n",
        "    StructField(\"store_id\", StringType(), True),\n",
        "    StructField(\"store_location\", StringType(), True),\n",
        "    StructField(\"product_id\", StringType(), True),\n",
        "    StructField(\"unit_price\", StringType(), True),\n",
        "    StructField(\"product_category\", StringType(), True),\n",
        "    StructField(\"product_type\", StringType(), True),\n",
        "    StructField(\"product_detail\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Use the schema for your streaming DataFrame\n",
        "stream_df = spark.readStream \\\n",
        "    .format(\"json\") \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"maxFilesPerTrigger\", 1) \\\n",
        "    .option(\"path\", \"/content/drive/MyDrive/stream_json\") \\\n",
        "    .load()\n"
      ],
      "metadata": {
        "id": "TCicdJSBkgw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"isStreaming:{stream_df.isStreaming}\")"
      ],
      "metadata": {
        "id": "XfI5nMaB1IF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(stream_df)"
      ],
      "metadata": {
        "id": "os3p6SNqU5-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch mode (reads all files immediately)\n",
        "batch_df = spark.read.schema(schema).json(\"/content/drive/MyDrive/stream_json\")\n",
        "batch_df.show(20, truncate=False)\n"
      ],
      "metadata": {
        "id": "-SsnfzH3tQ1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "stream_df = stream_df.withColumn(\"transaction_id\", col(\"transaction_id\").cast(\"int\")) \\\n",
        "                     .withColumn(\"transaction_qty\", col(\"transaction_qty\").cast(\"int\")) \\\n",
        "                     .withColumn(\"product_id\", col(\"product_id\").cast(\"int\")) \\\n",
        "                     .withColumn(\"unit_price\", col(\"unit_price\").cast(\"float\"))\n"
      ],
      "metadata": {
        "id": "ynb8vdZHugPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stateless vs Statful\n",
        "stateless:no memory/ex:select&filter/process each record independtly\\\n",
        "stateful:maintain information across batches/reauire checkpoint location/ex:groupBy&join&dropDuplicates/window operations(covered later)"
      ],
      "metadata": {
        "id": "YQNPt2fmS0v9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Streaming aggregations:\n",
        "count/sum&avg/min&max\n",
        "approx_count_distinct()\n",
        "aggregate based on window"
      ],
      "metadata": {
        "id": "ujEiqDjxXOfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Window operations\n",
        "Tumbling Windows :fixed (ex:counts every 5 min)\\\n",
        "Sliding windows :Overlapping windos where an event can belong to multiple windows\\\n",
        "Session windows:dynamically sized windows based on user activity (with gaps or session timeouts)"
      ],
      "metadata": {
        "id": "-8BxHVUkXizA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wq3UaWC_vJO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CQNbjwX7IFcv"
      }
    }
  ]
}